{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                  EDA_ users_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importamos las librerias necesarias para la lectura \n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "file = './australian_users_items.json'\n",
    "\n",
    "data = []\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            ### Usamos ast.literal_eval para interpretar la línea como un diccionario de Python\n",
    "            data_dict = ast.literal_eval(line)\n",
    "            data.append(data_dict)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error al interpretar la línea: {e}\")\n",
    "            break  \n",
    "\n",
    "# Convierte la lista de diccionarios a un DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88310 entries, 0 to 88309\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   user_id      88310 non-null  object\n",
      " 1   items_count  88310 non-null  int64 \n",
      " 2   steam_id     88310 non-null  object\n",
      " 3   user_url     88310 non-null  object\n",
      " 4   items        88310 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>items_count</th>\n",
       "      <th>steam_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76561197970982479</td>\n",
       "      <td>277</td>\n",
       "      <td>76561197970982479</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561197970...</td>\n",
       "      <td>[{'item_id': '10', 'item_name': 'Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>js41637</td>\n",
       "      <td>888</td>\n",
       "      <td>76561198035864385</td>\n",
       "      <td>http://steamcommunity.com/id/js41637</td>\n",
       "      <td>[{'item_id': '10', 'item_name': 'Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evcentric</td>\n",
       "      <td>137</td>\n",
       "      <td>76561198007712555</td>\n",
       "      <td>http://steamcommunity.com/id/evcentric</td>\n",
       "      <td>[{'item_id': '1200', 'item_name': 'Red Orchest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Riot-Punch</td>\n",
       "      <td>328</td>\n",
       "      <td>76561197963445855</td>\n",
       "      <td>http://steamcommunity.com/id/Riot-Punch</td>\n",
       "      <td>[{'item_id': '10', 'item_name': 'Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doctr</td>\n",
       "      <td>541</td>\n",
       "      <td>76561198002099482</td>\n",
       "      <td>http://steamcommunity.com/id/doctr</td>\n",
       "      <td>[{'item_id': '300', 'item_name': 'Day of Defea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id  items_count           steam_id  \\\n",
       "0  76561197970982479          277  76561197970982479   \n",
       "1            js41637          888  76561198035864385   \n",
       "2          evcentric          137  76561198007712555   \n",
       "3         Riot-Punch          328  76561197963445855   \n",
       "4              doctr          541  76561198002099482   \n",
       "\n",
       "                                            user_url  \\\n",
       "0  http://steamcommunity.com/profiles/76561197970...   \n",
       "1               http://steamcommunity.com/id/js41637   \n",
       "2             http://steamcommunity.com/id/evcentric   \n",
       "3            http://steamcommunity.com/id/Riot-Punch   \n",
       "4                 http://steamcommunity.com/id/doctr   \n",
       "\n",
       "                                               items  \n",
       "0  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
       "1  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
       "2  [{'item_id': '1200', 'item_name': 'Red Orchest...  \n",
       "3  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
       "4  [{'item_id': '300', 'item_name': 'Day of Defea...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             user_id item_id                  item_name  playtime_forever  \\\n",
      "0  76561197970982479      10             Counter-Strike                 6   \n",
      "1  76561197970982479      20      Team Fortress Classic                 0   \n",
      "2  76561197970982479      30              Day of Defeat                 7   \n",
      "3  76561197970982479      40         Deathmatch Classic                 0   \n",
      "4  76561197970982479      50  Half-Life: Opposing Force                 0   \n",
      "\n",
      "   playtime_2weeks  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n"
     ]
    }
   ],
   "source": [
    "### Función para extraer información de cada juego en la lista 'items'\n",
    "def extract_game_data(items_list, user_id):\n",
    "    \n",
    "    user_data = []\n",
    "    \n",
    "    ### Iterar sobre cada juego en la lista de 'items'\n",
    "    for item in items_list:\n",
    "        ### Extraer información del juego\n",
    "        game_data = {\n",
    "            'user_id': user_id,\n",
    "            'item_id': item.get('item_id', ''),\n",
    "            'item_name': item.get('item_name', ''),\n",
    "            'playtime_forever': item.get('playtime_forever', 0),\n",
    "            'playtime_2weeks': item.get('playtime_2weeks', 0)\n",
    "        }\n",
    "        user_data.append(game_data)\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "### Lista para guardar los datos desanidados de todos los usuarios\n",
    "all_data = []\n",
    "\n",
    "### Iterar sobre cada fila del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    ### Aplicar la función de desanidamiento\n",
    "    user_games = extract_game_data(row['items'], row['user_id'])\n",
    "    all_data.extend(user_games)\n",
    "\n",
    "### Convertir la lista de diccionarios a DataFrame\n",
    "df_desanidado = pd.DataFrame(all_data)\n",
    "\n",
    "### Ahora 'df_desanidado' contiene la información desanidada\n",
    "print(df_desanidado.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf504bdf6584a10817e1fa52a39b2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c076dcb7ea48c598120cb0502af7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1a8dc69e6f414391e5ac968fdfa94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3962dfac63b24ba58be4443122d20755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe guardado exitosamente en: C:\\Users\\ASUS\\PI_ML_OPS\\PI_ML_OPS-\\INFORMES GENERADOS PARA EL DATASET\\informe_australian_users_items.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "import ast\n",
    "\n",
    "file_items = './australian_users_items.json'\n",
    "data_items = []\n",
    "\n",
    "with open(file_items, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data_dict = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                data_dict = ast.literal_eval(line)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error al interpretar la línea: {e}\")\n",
    "                continue\n",
    "        data_items.append(data_dict)\n",
    "\n",
    "df_items = pd.DataFrame(data_items)\n",
    "\n",
    "def extract_item_data_optimized(row):\n",
    "    items_list = row.get('items', [])\n",
    "    if items_list:\n",
    "        return [\n",
    "            [item.get('item_id', ''), item.get('item_name', ''), item.get('playtime_forever', 0), item.get('playtime_2weeks', 0)]\n",
    "            for item in items_list\n",
    "        ]\n",
    "    return [None] * 4\n",
    "\n",
    "extracted_data = df_items.apply(extract_item_data_optimized, axis=1)\n",
    "normalized_data = [item for sublist in extracted_data for item in sublist if item]\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=['item_id', 'item_name', 'playtime_forever', 'playtime_2weeks'])\n",
    "\n",
    "df_normalized.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Generar el informe de df_normalized\n",
    "profile_reviews = ProfileReport(df_normalized, minimal=True)\n",
    "\n",
    "# Ruta especificada para guardar el informe\n",
    "ruta_informe_reviews = \"C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\INFORMES GENERADOS PARA EL DATASET\\\\informe_australian_users_items.html\"\n",
    "\n",
    "profile_reviews.to_file(ruta_informe_reviews)\n",
    "\n",
    "print(f\"Informe guardado exitosamente en: {ruta_informe_reviews}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             user_id  items_count           steam_id  \\\n",
      "0  76561197970982479          277  76561197970982479   \n",
      "1            js41637          888  76561198035864385   \n",
      "2          evcentric          137  76561198007712555   \n",
      "3         Riot-Punch          328  76561197963445855   \n",
      "4              doctr          541  76561198002099482   \n",
      "\n",
      "                                            user_url  \\\n",
      "0  http://steamcommunity.com/profiles/76561197970...   \n",
      "1               http://steamcommunity.com/id/js41637   \n",
      "2             http://steamcommunity.com/id/evcentric   \n",
      "3            http://steamcommunity.com/id/Riot-Punch   \n",
      "4                 http://steamcommunity.com/id/doctr   \n",
      "\n",
      "                                               items  \n",
      "0  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
      "1  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
      "2  [{'item_id': '1200', 'item_name': 'Red Orchest...  \n",
      "3  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
      "4  [{'item_id': '300', 'item_name': 'Day of Defea...  \n",
      "Archivo guardado en: C:\\Users\\ASUS\\PI_ML_OPS\\PI_ML_OPS-\\dataAPI\\dataframe_user_items_combinado.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import webbrowser\n",
    "\n",
    "file_items = './australian_users_items.json'\n",
    "data_items = []\n",
    "\n",
    "# Intentar leer el archivo línea por línea usando ast.literal_eval\n",
    "with open(file_items, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Usar ast.literal_eval para convertir la línea en un diccionario\n",
    "            data_dict = ast.literal_eval(line)\n",
    "            data_items.append(data_dict)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error al interpretar la línea: {e}\")\n",
    "\n",
    "# Verificar si se cargaron datos\n",
    "if not data_items:\n",
    "    print(\"No se cargaron datos desde el archivo.\")\n",
    "else:\n",
    "    df = pd.DataFrame(data_items)\n",
    "    print(df.head())  # Mostrar las primeras filas para verificar\n",
    "\n",
    "    # Función para extraer datos de juego de cada usuario\n",
    "    def extract_game_data(items_list, user_id):\n",
    "        user_data = []\n",
    "        for item in items_list:\n",
    "            game_data = {\n",
    "                'user_id': user_id,\n",
    "                'item_id': item.get('item_id', ''),\n",
    "                'playtime_forever': item.get('playtime_forever', 0)\n",
    "            }\n",
    "            user_data.append(game_data)\n",
    "        return user_data\n",
    "\n",
    "    all_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        user_games = extract_game_data(row.get('items', []), row.get('user_id', ''))\n",
    "        all_data.extend(user_games)\n",
    "\n",
    "    df_combinado = pd.DataFrame(all_data)\n",
    "\n",
    "    # Guardar el DataFrame combinado en formato JSON\n",
    "    ruta_guardado = \"C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_items_combinado.json\"\n",
    "    df_combinado.to_json(ruta_guardado, orient='records', lines=True)\n",
    "\n",
    "    # Utilizar webbrowser para abrir el archivo JSON en el navegador\n",
    "    webbrowser.open('file://' + ruta_guardado)\n",
    "\n",
    "    print(f\"Archivo guardado en: {ruta_guardado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\asus\\anaconda3\\lib\\site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyarrow) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip  install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\asus\\anaconda3\\lib\\site-packages (2023.10.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fastparquet) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fastparquet) (22.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fastparquet) (2.8.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fastparquet) (2022.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado en: C:\\Users\\ASUS\\PI_ML_OPS\\PI_ML_OPS-\\dataAPI\\combined_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rutas de los archivos\n",
    "ruta_combined = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_steam_game_cleaned_data.json'\n",
    "ruta_items = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_items_combinado.json'\n",
    "ruta_reviews = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_reviews_combinado.json'\n",
    "\n",
    "# Cargar los archivos JSON en DataFrames\n",
    "df_combined = pd.read_json(ruta_combined, lines=True)\n",
    "df_items = pd.read_json(ruta_items, lines=True)\n",
    "df_reviews = pd.read_json(ruta_reviews, lines=True)\n",
    "\n",
    "# Función para verificar y renombrar columnas duplicadas\n",
    "def add_prefix_if_needed(*dfs):\n",
    "    all_columns = [col for df in dfs for col in df.columns]\n",
    "    duplicates = set(col for col in all_columns if all_columns.count(col) > 1)\n",
    "    for i, df in enumerate(dfs):\n",
    "        intersecting_columns = set(df.columns) & duplicates\n",
    "        if intersecting_columns:\n",
    "            prefix = ['combined_', 'items_', 'reviews_'][i]\n",
    "            df.rename(columns={col: f\"{prefix}{col}\" for col in intersecting_columns}, inplace=True)\n",
    "\n",
    "# Verificar y renombrar columnas duplicadas si es necesario\n",
    "add_prefix_if_needed(df_combined, df_items, df_reviews)\n",
    "\n",
    "# Tomar una muestra aleatoria o las primeras 'n' filas de cada DataFrame\n",
    "n = 1000  # Ajustar según la necesidad\n",
    "df_combined_sample = df_combined.head(n)\n",
    "df_items_sample = df_items.head(n)\n",
    "df_reviews_sample = df_reviews.head(n)\n",
    "\n",
    "# Combinar los DataFrames\n",
    "df_final = pd.concat([df_combined_sample, df_items_sample, df_reviews_sample], axis=1)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo JSON\n",
    "ruta_guardado = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_data.json'\n",
    "df_final.to_json(ruta_guardado, orient='records', lines=True)\n",
    "\n",
    "print(f\"Archivo combinado guardado en: {ruta_guardado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo comprimido guardado en: C:\\Users\\ASUS\\PI_ML_OPS\\PI_ML_OPS-\\dataAPI\\combined_data.json.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Ruta del archivo original y ruta del archivo comprimido\n",
    "ruta_archivo_original = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_data.json'\n",
    "ruta_archivo_comprimido = ruta_archivo_original + '.gz'\n",
    "\n",
    "# Comprimir el archivo en formato gzip\n",
    "with open(ruta_archivo_original, 'rb') as original_file:\n",
    "    with gzip.open(ruta_archivo_comprimido, 'wb') as compressed_file:\n",
    "        shutil.copyfileobj(original_file, compressed_file)\n",
    "\n",
    "print(f\"Archivo comprimido guardado en: {ruta_archivo_comprimido}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'combined_price' existe en df_combined.\n",
      "0            4.99\n",
      "1    Free To Play\n",
      "2    Free to Play\n",
      "3            0.99\n",
      "4            2.99\n",
      "Name: combined_price, dtype: object\n",
      "'combined_price' existe en df_final.\n",
      "0            4.99\n",
      "1    Free To Play\n",
      "2    Free to Play\n",
      "3            0.99\n",
      "4            2.99\n",
      "Name: combined_price, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Asume que el DataFrame df_combined ya ha sido cargado con los datos de 'combined_steam_game_cleaned_data.json'\n",
    "\n",
    "# Verificar si 'combined_price' existe en df_combined\n",
    "if 'combined_price' in df_combined.columns:\n",
    "    print(\"'combined_price' existe en df_combined.\")\n",
    "    # Mostrar las primeras filas de la columna 'combined_price'\n",
    "    print(df_combined['combined_price'].head())\n",
    "else:\n",
    "    print(\"'combined_price' no se encuentra en df_combined.\")\n",
    "\n",
    "# Si estás trabajando con df_final después de combinar df_combined, df_items, y df_reviews\n",
    "if 'combined_price' in df_final.columns:\n",
    "    print(\"'combined_price' existe en df_final.\")\n",
    "    # Mostrar las primeras filas de la columna 'combined_price'\n",
    "    print(df_final['combined_price'].head())\n",
    "else:\n",
    "    print(\"'combined_price' no se encuentra en df_final.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['developer', 'release_date', 'price', 'early_access', 'id', 'genres'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_combined.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado en formato Parquet en: C:\\Users\\ASUS\\PI_ML_OPS\\PI_ML_OPS-\\dataAPI\\combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Asegúrate de que las rutas sean correctas y reemplázalas con las rutas reales de tus archivos\n",
    "ruta_combined = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_steam_game_cleaned_data.json'\n",
    "ruta_items = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_items_combinado.json'\n",
    "ruta_reviews = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_reviews_combinado.json'\n",
    "\n",
    "# Cargar los archivos JSON en DataFrames\n",
    "df_combined = pd.read_json(ruta_combined, lines=True)\n",
    "df_items = pd.read_json(ruta_items, lines=True)\n",
    "df_reviews = pd.read_json(ruta_reviews, lines=True)\n",
    "\n",
    "# Transformar 'price' a numérico, considerando 'Free To Play' y 'Free to Play' como '0'\n",
    "df_combined['price'] = df_combined['price'].replace(['Free To Play', 'Free to Play'], '0')\n",
    "df_combined['price'] = pd.to_numeric(df_combined['price'], errors='coerce').fillna(0)\n",
    "\n",
    "# Añadir prefijos a las columnas para evitar confusiones después de manejar 'price'\n",
    "df_combined = df_combined.add_prefix('combined_')\n",
    "df_items = df_items.add_prefix('items_')\n",
    "df_reviews = df_reviews.add_prefix('reviews_')\n",
    "\n",
    "# Combinar los DataFrames\n",
    "df_final = pd.concat([df_combined, df_items, df_reviews], axis=1)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo en formato Parquet\n",
    "ruta_guardado_parquet = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_data.parquet'\n",
    "df_final.to_parquet(ruta_guardado_parquet, engine='pyarrow')\n",
    "\n",
    "print(f\"Archivo combinado guardado en formato Parquet en: {ruta_guardado_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo Parquet cargado correctamente.\n",
      "Todas las columnas requeridas están presentes en el DataFrame.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo Parquet (asegúrate de reemplazarla con tu ruta real)\n",
    "ruta_archivo_parquet = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_data.parquet'\n",
    "\n",
    "try:\n",
    "    # Cargar el DataFrame desde el archivo Parquet\n",
    "    df = pd.read_parquet(ruta_archivo_parquet)\n",
    "    print(\"Archivo Parquet cargado correctamente.\")\n",
    "\n",
    "    # Lista de columnas requeridas mencionadas en el código\n",
    "    columnas_requeridas = [\n",
    "        'combined_release_date',\n",
    "        'combined_price',\n",
    "        'combined_developer',\n",
    "        # Añade aquí cualquier otra columna requerida\n",
    "    ]\n",
    "\n",
    "    # Verificar si todas las columnas requeridas existen en el DataFrame\n",
    "    columnas_faltantes = [col for col in columnas_requeridas if col not in df.columns]\n",
    "    if not columnas_faltantes:\n",
    "        print(\"Todas las columnas requeridas están presentes en el DataFrame.\")\n",
    "    else:\n",
    "        print(f\"Faltan las siguientes columnas en el DataFrame: {columnas_faltantes}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el archivo Parquet: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado en: C:\\Users\\ASUS\\PI_ML_OPS\\PI_ML_OPS-\\dataAPI\\combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rutas de los archivos\n",
    "ruta_combined = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_steam_game_cleaned_data.json'\n",
    "ruta_items = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_items_combinado.json'\n",
    "ruta_reviews = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\dataframe_user_reviews_combinado.json'\n",
    "\n",
    "# Cargar los archivos JSON en DataFrames\n",
    "df_combined = pd.read_json(ruta_combined, lines=True)\n",
    "df_items = pd.read_json(ruta_items, lines=True)\n",
    "df_reviews = pd.read_json(ruta_reviews, lines=True)\n",
    "\n",
    "# Renombrar las columnas agregando prefijos\n",
    "df_combined = df_combined.add_prefix('combined_')\n",
    "df_items = df_items.add_prefix('items_')\n",
    "df_reviews = df_reviews.add_prefix('reviews_')\n",
    "\n",
    "# Tomar una muestra aleatoria o las primeras 'n' filas de cada DataFrame\n",
    "n = 200  # Ajustar según la necesidad\n",
    "df_combined_sample = df_combined.head(n)\n",
    "df_items_sample = df_items.head(n)\n",
    "df_reviews_sample = df_reviews.head(n)\n",
    "\n",
    "# Combinar los DataFrames\n",
    "df_final = pd.concat([df_combined_sample, df_items_sample, df_reviews_sample], axis=1)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo CSV\n",
    "ruta_guardado_csv = 'C:\\\\Users\\\\ASUS\\\\PI_ML_OPS\\\\PI_ML_OPS-\\\\dataAPI\\\\combined_data.csv'\n",
    "df_final.to_csv(ruta_guardado_csv, index=False)\n",
    "\n",
    "print(f\"Archivo combinado guardado en: {ruta_guardado_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
